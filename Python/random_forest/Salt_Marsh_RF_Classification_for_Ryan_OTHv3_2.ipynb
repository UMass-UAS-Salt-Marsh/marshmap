{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmknlbLyPF2J"
   },
   "source": [
    "\n",
    "### This version does not use dask to load imagery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M2Gz90bpcNj3",
    "outputId": "9c819ed1-6036-40ba-c437-64e4f491c962"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "#comment test, 15 Sep 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hlWn28OfcTGq"
   },
   "outputs": [],
   "source": [
    "from osgeo import gdal, ogr, gdal_array # I/O image data\n",
    "import numpy as np # math and array handling\n",
    "import matplotlib.pyplot as plt # plot figures\n",
    "from sklearn.ensemble import RandomForestClassifier # classifier\n",
    "import pandas as pd # handling large data as table sheets\n",
    "from sklearn.metrics import classification_report, accuracy_score,confusion_matrix  # calculating measures for accuracy assessment\n",
    "\n",
    "import seaborn as sn\n",
    "\n",
    "import datetime\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uPX1k_NX5cc-"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# print(os.environ['GDAL_NUM_THREADS'])\n",
    "os.environ['PROJ_LIB'] = \"/work/pi_gstuart_umass_edu/kate/conda/share/proj\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bneZO374zh9x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available attributes in the shape file are: ['fid', 'id', 'left', 'top', 'right', 'bottom', 'Raw Subcla', 'Transect', 'PointNum', 'SubClass', 'Northing', 'Easting', 'Altitude', 'Notes', 'Class', 'Pre Angle', 'Post Angle', 'mu', 'Class mu', 'Pre/Post', 'Hydro', 'ReclassV2', 'layer', 'path', 'area', 'ReclassV3', 'ReClass v4']\n"
     ]
    }
   ],
   "source": [
    "# Tell GDAL to throw Python exceptions, and register all drivers\n",
    "gdal.UseExceptions()\n",
    "gdal.AllRegister()\n",
    "\n",
    "# define a number of trees that should be used (default = 500)\n",
    "est = 500\n",
    "\n",
    "# how many cores should be used?\n",
    "# -1 -> all available cores\n",
    "n_cores = 60\n",
    "\n",
    "# the remote sensing image you want to classify\n",
    "img_RS = r'/work/pi_cschweik_umass_edu/Ryan/Modeling_stack/Mid_High_Stacked_Clipped.tif'\n",
    "\n",
    "# training and validation as shape files\n",
    "# training = r'/work/pi_gstuart_umass_edu/kate/OTH_Training_Data/OTH_Polygons_v3_relassed.shp'\n",
    "# validation = r'/work/pi_gstuart_umass_edu/kate/OTH_Training_Data/OTH_Polygons_v3_relassed.shp'\n",
    "\n",
    "training = r'/work/pi_cschweik_umass_edu/Ryan/OTH_Ground_Truth_Data/Removed or Edited/OTH_Poygons_withTrees_06Nov2023_4326.shp'\n",
    "validation = r'/work/pi_cschweik_umass_edu/Ryan/OTH_Ground_Truth_Data/Removed or Edited/OTH_Poygons_withTrees_06Nov2023_4326.shp'\n",
    "\n",
    "# what is the attributes name of your classes in the shape file (field name of the classes)?\n",
    "attribute = 'ReClassV3'\n",
    "\n",
    "# directory, where the classification image should be saved:\n",
    "# classification_image = r'/work/pi_gstuart_umass_edu/kate/Classification_products/OTH_all_reclass_classification_v1.gtif'\n",
    "classification_image = r'/work/pi_cschweik_umass_edu/Ryan/Classification_products/OTH_all_reclass_classification_v3_s2.gtif'\n",
    "\n",
    "# directory, where the all meta results should be saved:\n",
    "# results_txt = r'/work/pi_gstuart_umass_edu/kate/Classification_products/OTH_all_reclass_classification_v1.txt'\n",
    "results_txt = r'/work/pi_cschweik_umass_edu/Ryan/Classification_products/OTH_all_reclass_classification_v3_s2.txt'\n",
    "\n",
    "# laod training data and show all shape attributes\n",
    "\n",
    "#model_dataset = gdal.Open(model_raster_fname)\n",
    "shape_dataset = ogr.Open(training)\n",
    "shape_layer = shape_dataset.GetLayer()\n",
    "\n",
    "# extract the names of all attributes (fieldnames) in the shape file\n",
    "attributes = []\n",
    "ldefn = shape_layer.GetLayerDefn()\n",
    "for n in range(ldefn.GetFieldCount()):\n",
    "    fdefn = ldefn.GetFieldDefn(n)\n",
    "    attributes.append(fdefn.name)\n",
    "    \n",
    "# print the attributes\n",
    "print('Available attributes in the shape file are: {}'.format(attributes))\n",
    "\n",
    "# prepare results text file:\n",
    "\n",
    "print('Random Forest Classification', file=open(results_txt, \"a\"))\n",
    "print('Processing: {}'.format(datetime.datetime.now()), file=open(results_txt, \"a\"))\n",
    "print('-------------------------------------------------', file=open(results_txt, \"a\"))\n",
    "print('PATHS:', file=open(results_txt, \"a\"))\n",
    "print('Image: {}'.format(img_RS), file=open(results_txt, \"a\"))\n",
    "print('Training shape: {}'.format(training) , file=open(results_txt, \"a\"))\n",
    "print('Vaildation shape: {}'.format(validation) , file=open(results_txt, \"a\"))\n",
    "print('      choosen attribute: {}'.format(attribute) , file=open(results_txt, \"a\"))\n",
    "print('Classification image: {}'.format(classification_image) , file=open(results_txt, \"a\"))\n",
    "print('Report text file: {}'.format(results_txt) , file=open(results_txt, \"a\"))\n",
    "print('-------------------------------------------------', file=open(results_txt, \"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bneZO374zh9x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11908, 13206, 103) <class 'numpy.uint16'> 2\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "Image extent: 11908 x 13206 (row x col)\n",
      "Number of Bands: 103\n"
     ]
    }
   ],
   "source": [
    "# load image data\n",
    "######THIS IS STEP THAT TAKES A LONG TIME#######\n",
    "\n",
    "img_ds = gdal.Open(img_RS, gdal.GA_ReadOnly)\n",
    "\n",
    "img = np.zeros((img_ds.RasterYSize, img_ds.RasterXSize, img_ds.RasterCount),\n",
    "               gdal_array.GDALTypeCodeToNumericTypeCode(img_ds.GetRasterBand(1).DataType))\n",
    "print(img.shape, gdal_array.GDALTypeCodeToNumericTypeCode(img_ds.GetRasterBand(1).DataType), img.itemsize)\n",
    "print(img_ds.GetRasterBand(1).ReadAsArray().itemsize)\n",
    "for b in range(img.shape[2]):\n",
    "    print(b)\n",
    "    img[:, :, b] = img_ds.GetRasterBand(b + 1).ReadAsArray()\n",
    "    \n",
    "row = img_ds.RasterYSize\n",
    "col = img_ds.RasterXSize\n",
    "band_number = img_ds.RasterCount\n",
    "\n",
    "print('Image extent: {} x {} (row x col)'.format(row, col))\n",
    "print('Number of Bands: {}'.format(band_number))\n",
    "\n",
    "\n",
    "print('Image extent: {} x {} (row x col)'.format(row, col), file=open(results_txt, \"a\"))\n",
    "print('Number of Bands: {}'.format(band_number), file=open(results_txt, \"a\"))\n",
    "print('---------------------------------------', file=open(results_txt, \"a\"))\n",
    "print('TRAINING', file=open(results_txt, \"a\"))\n",
    "print('Number of Trees: {}'.format(est), file=open(results_txt, \"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bneZO374zh9x"
   },
   "outputs": [],
   "source": [
    "# laod training data from shape file\n",
    "# img_ds = gdal.Open(img_RS, gdal.GA_ReadOnly)\n",
    "\n",
    "#model_dataset = gdal.Open(model_raster_fname)\n",
    "shape_dataset = ogr.Open(training)\n",
    "shape_layer = shape_dataset.GetLayer()\n",
    "\n",
    "mem_drv = gdal.GetDriverByName('MEM')\n",
    "mem_raster = mem_drv.Create('',img_ds.RasterXSize,img_ds.RasterYSize,1,gdal.GDT_UInt16)\n",
    "#mem_raster.SetProjection(img_ds.GetProjection())\n",
    "mem_raster.SetGeoTransform(img_ds.GetGeoTransform())\n",
    "mem_band = mem_raster.GetRasterBand(1)\n",
    "mem_band.Fill(0)\n",
    "mem_band.SetNoDataValue(0)\n",
    "\n",
    "att_ = 'ATTRIBUTE='+attribute\n",
    "# http://gdal.org/gdal__alg_8h.html#adfe5e5d287d6c184aab03acbfa567cb1\n",
    "# http://gis.stackexchange.com/questions/31568/gdal-rasterizelayer-doesnt-burn-all-polygons-to-raster\n",
    "err = gdal.RasterizeLayer(mem_raster, [1], shape_layer, None, None, [1],  [att_,\"ALL_TOUCHED=TRUE\"])\n",
    "assert err == gdal.CE_None\n",
    "\n",
    "roi = mem_raster.ReadAsArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bneZO374zh9x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "707591 training samples\n",
      "training data include 6 classes: [  1   2   3  13  99 100]\n",
      "Our X matrix is sized: (707591, 103)\n",
      "Our y array is sized: (707591,)\n"
     ]
    }
   ],
   "source": [
    "# # Display images\n",
    "# plt.subplot(121)\n",
    "# plt.imshow(img[:, :, 0], cmap=plt.cm.Greys_r)\n",
    "# plt.title('RS image - first band')\n",
    "\n",
    "# plt.subplot(122)\n",
    "# plt.imshow(roi, cmap=plt.cm.Spectral)\n",
    "# plt.title('Training Image')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# Number of training pixels:\n",
    "n_samples = (roi > 0).sum()\n",
    "print('{n} training samples'.format(n=n_samples))\n",
    "print('{n} training samples'.format(n=n_samples), file=open(results_txt, \"a\"))\n",
    "\n",
    "# What are our classification labels?\n",
    "labels = np.unique(roi[roi > 0])\n",
    "print('training data include {n} classes: {classes}'.format(n=labels.size, classes=labels))\n",
    "print('training data include {n} classes: {classes}'.format(n=labels.size, classes=labels), file=open(results_txt, \"a\"))\n",
    "\n",
    "# Subset the image dataset with the training image = X\n",
    "# Mask the classes on the training dataset = y\n",
    "# These will have n_samples rows\n",
    "X = img[roi > 0, :]\n",
    "y = roi[roi > 0]\n",
    "\n",
    "print('Our X matrix is sized: {sz}'.format(sz=X.shape))\n",
    "print('Our y array is sized: {sz}'.format(sz=y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bneZO374zh9x"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend ThreadingBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  80 tasks      | elapsed:   19.1s\n",
      "[Parallel(n_jobs=60)]: Done 330 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=60)]: Done 500 out of 500 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model pickled\n",
      "OOB prediction of accuracy is: 98.87124058954961%\n",
      "Band 1 importance: 0.015130369447407348\n",
      "Band 2 importance: 0.020097316811893674\n",
      "Band 3 importance: 0.015252093113639438\n",
      "Band 4 importance: 0.008606753216610868\n",
      "Band 5 importance: 0.008920666620921546\n",
      "Band 6 importance: 0.018016747847878777\n",
      "Band 7 importance: 0.008723486866708537\n",
      "Band 8 importance: 0.006001505014861539\n",
      "Band 9 importance: 0.0038643595145276743\n",
      "Band 10 importance: 0.026141171841620948\n",
      "Band 11 importance: 0.009820692641197542\n",
      "Band 12 importance: 0.004094610325419305\n",
      "Band 13 importance: 0.010843167966968612\n",
      "Band 14 importance: 0.0032552397679908225\n",
      "Band 15 importance: 0.0049388029607229655\n",
      "Band 16 importance: 0.003040646842026826\n",
      "Band 17 importance: 0.005210452111392184\n",
      "Band 18 importance: 0.0074523021634540715\n",
      "Band 19 importance: 0.005325603419165448\n",
      "Band 20 importance: 0.004171075285452449\n",
      "Band 21 importance: 0.005060745171782924\n",
      "Band 22 importance: 0.00553278582625066\n",
      "Band 23 importance: 0.005234034941652306\n",
      "Band 24 importance: 0.017702407763792984\n",
      "Band 25 importance: 0.012126819683647764\n",
      "Band 26 importance: 0.014437905898552417\n",
      "Band 27 importance: 0.005180737509982228\n",
      "Band 28 importance: 0.0061519620665366126\n",
      "Band 29 importance: 0.005273717870183733\n",
      "Band 30 importance: 0.0051167128425492515\n",
      "Band 31 importance: 0.007620764203737327\n",
      "Band 32 importance: 0.004636917975114875\n",
      "Band 33 importance: 0.00527129063117928\n",
      "Band 34 importance: 0.006277341687220046\n",
      "Band 35 importance: 0.006495036961249359\n",
      "Band 36 importance: 0.004865401413171346\n",
      "Band 37 importance: 0.0052973248550362475\n",
      "Band 38 importance: 0.004905409548690553\n",
      "Band 39 importance: 0.005524468099399867\n",
      "Band 40 importance: 0.011048502902675695\n",
      "Band 41 importance: 0.010507856961345631\n",
      "Band 42 importance: 0.010888702955225054\n",
      "Band 43 importance: 0.006393683317749199\n",
      "Band 44 importance: 0.007216204042582497\n",
      "Band 45 importance: 0.0037193993477283474\n",
      "Band 46 importance: 0.003991373983593052\n",
      "Band 47 importance: 0.004763999997971048\n",
      "Band 48 importance: 0.006224302263923673\n",
      "Band 49 importance: 0.0038400375743428985\n",
      "Band 50 importance: 0.007460108203541798\n",
      "Band 51 importance: 0.008431437819548984\n",
      "Band 52 importance: 0.007321962035653228\n",
      "Band 53 importance: 0.010368440856562062\n",
      "Band 54 importance: 0.009838872184112933\n",
      "Band 55 importance: 0.006102609346634015\n",
      "Band 56 importance: 0.004220930104498286\n",
      "Band 57 importance: 0.0038223935935625232\n",
      "Band 58 importance: 0.013652659093706266\n",
      "Band 59 importance: 0.028582561473740355\n",
      "Band 60 importance: 0.0068852719417024635\n",
      "Band 61 importance: 0.007815902849714163\n",
      "Band 62 importance: 0.005825975760050102\n",
      "Band 63 importance: 0.007233524467880289\n",
      "Band 64 importance: 0.005939521133534736\n",
      "Band 65 importance: 0.015882795100088505\n",
      "Band 66 importance: 0.012749895272109346\n",
      "Band 67 importance: 0.012623021557630166\n",
      "Band 68 importance: 0.011851529570015557\n",
      "Band 69 importance: 0.01387197739560798\n",
      "Band 70 importance: 0.01261700544643183\n",
      "Band 71 importance: 0.013392759428456424\n",
      "Band 72 importance: 0.02090117828522394\n",
      "Band 73 importance: 0.009281449716171208\n",
      "Band 74 importance: 0.009809267914824199\n",
      "Band 75 importance: 0.010186331407932495\n",
      "Band 76 importance: 0.015851862526907182\n",
      "Band 77 importance: 0.008164751022091302\n",
      "Band 78 importance: 0.007349873541769977\n",
      "Band 79 importance: 0.007185073763428657\n",
      "Band 80 importance: 0.006906592635019047\n",
      "Band 81 importance: 0.00880892680455177\n",
      "Band 82 importance: 0.010779014570018128\n",
      "Band 83 importance: 0.012214151158621048\n",
      "Band 84 importance: 0.010962556918629525\n",
      "Band 85 importance: 0.011205772944195214\n",
      "Band 86 importance: 0.005526780605450201\n",
      "Band 87 importance: 0.007515349488625532\n",
      "Band 88 importance: 0.005196444878043867\n",
      "Band 89 importance: 0.006869383705265316\n",
      "Band 90 importance: 0.015209352246149655\n",
      "Band 91 importance: 0.014732880361802795\n",
      "Band 92 importance: 0.006941605473710903\n",
      "Band 93 importance: 0.021989538275449947\n",
      "Band 94 importance: 0.024364696642678766\n",
      "Band 95 importance: 0.03788755033188454\n",
      "Band 96 importance: 0.014954794574101374\n",
      "Band 97 importance: 0.009310972376505324\n",
      "Band 98 importance: 0.007154451135504232\n",
      "Band 99 importance: 0.007141144872860739\n",
      "Band 100 importance: 0.006909460103542253\n",
      "Band 101 importance: 0.0072787559685278235\n",
      "Band 102 importance: 0.010757089471802041\n",
      "Band 103 importance: 0.02394887956720146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend ThreadingBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  80 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=60)]: Done 330 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=60)]: Done 500 out of 500 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict       1    2       3    13     99     100     All\n",
      "truth                                                    \n",
      "1        160723    0       0     0      0       0  160723\n",
      "2             0  936       0     0      0       0     936\n",
      "3           990    0  123537     0      0       0  124527\n",
      "13          300    0       0  7337      0       0    7637\n",
      "99         2340    0       0     0  71316       0   73656\n",
      "100        3417    0       0     0      0  336695  340112\n",
      "All      167770  936  123537  7337  71316  336695  707591\n",
      "Reshaped from (11908, 13206, 103) to (157257048, 103)\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=est, oob_score=True, verbose=1, n_jobs=n_cores)\n",
    "\n",
    "# verbose = 2 -> prints out every tree progression\n",
    "# rf = RandomForestClassifier(n_estimators=est, oob_score=True, verbose=2, n_jobs=n_cores)\n",
    "\n",
    "\n",
    "\n",
    "X = np.nan_to_num(X)\n",
    "rf2 = rf.fit(X, y)\n",
    "\n",
    "\n",
    "# Save the trained model to a file\n",
    "pickle_file = '/work/pi_cschweik_umass_edu/Ryan/Classification_products/OTH_all_reclass_classification_v3_s2.pkl'\n",
    "with open(pickle_file, 'wb') as file:\n",
    "    pickle.dump(rf2, file)\n",
    "\n",
    "print ('model pickled')\n",
    "\n",
    "# With our Random Forest model fit, we can check out the \"Out-of-Bag\" (OOB) prediction score:\n",
    "\n",
    "print('--------------------------------', file=open(results_txt, \"a\"))\n",
    "print('TRAINING and RF Model Diagnostics:', file=open(results_txt, \"a\"))\n",
    "print('OOB prediction of accuracy is: {oob}%'.format(oob=rf.oob_score_ * 100))\n",
    "print('OOB prediction of accuracy is: {oob}%'.format(oob=rf.oob_score_ * 100), file=open(results_txt, \"a\"))\n",
    "\n",
    "\n",
    "# we can show the band importance:\n",
    "bands = range(1,img_ds.RasterCount+1)\n",
    "\n",
    "for b, imp in zip(bands, rf2.feature_importances_):\n",
    "    print('Band {b} importance: {imp}'.format(b=b, imp=imp))\n",
    "    print('Band {b} importance: {imp}'.format(b=b, imp=imp), file=open(results_txt, \"a\"))\n",
    "\n",
    "    \n",
    "# Let's look at a crosstabulation to see the class confusion. \n",
    "# To do so, we will import the Pandas library for some help:\n",
    "# Setup a dataframe -- just like R\n",
    "# Exception Handling because of possible Memory Error\n",
    "\n",
    "try:\n",
    "    df = pd.DataFrame()\n",
    "    df['truth'] = y\n",
    "    df['predict'] = rf.predict(X)\n",
    "\n",
    "except MemoryError:\n",
    "    print('Crosstab not available ')\n",
    "\n",
    "else:\n",
    "    # Cross-tabulate predictions\n",
    "    print(pd.crosstab(df['truth'], df['predict'], margins=True))\n",
    "    print(pd.crosstab(df['truth'], df['predict'], margins=True), file=open(results_txt, \"a\"))\n",
    "    \n",
    "# Predicting the rest of the image\n",
    "\n",
    "# generate mask image from red band\n",
    "mask = np.copy(img[:,:,0])\n",
    "mask[mask > 0.0] = 1.0 # all actual pixels have a value of 1.0\n",
    "\n",
    "# Take our full image and reshape into long 2d array (nrow * ncol, nband) for classification\n",
    "old_shape = img.shape\n",
    "new_shape = (img.shape[0] * img.shape[1], img.shape[2])\n",
    "# img = img[:, :, :int(img.shape[2])].reshape(new_shape)\n",
    "img = img.reshape(new_shape)\n",
    "\n",
    "print('Reshaped from {o} to {n}'.format(o=old_shape, n=img.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vrB7XKyDr3LH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "# img = np.nan_to_num(img)\n",
    "img[np.isnan(img)] = 0.0\n",
    "# class_prediction = rf.predict(img)\n",
    "\n",
    "print (\"DONE!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp8gwSplB0LJ"
   },
   "source": [
    "# **Apply Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "z4Sry-DPcVpQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend ThreadingBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  80 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=60)]: Done 330 tasks      | elapsed:   16.5s\n",
      "[Parallel(n_jobs=60)]: Done 500 out of 500 | elapsed:   23.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.999999745639381 %, derzeit: 7862852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend ThreadingBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  80 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=60)]: Done 330 tasks      | elapsed:   16.7s\n",
      "[Parallel(n_jobs=60)]: Done 500 out of 500 | elapsed:   23.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.999999491278762 %, derzeit: 15725704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend ThreadingBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  80 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=60)]: Done 330 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=60)]: Done 500 out of 500 | elapsed:   24.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.999999236918145 %, derzeit: 23588556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend ThreadingBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  80 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=60)]: Done 330 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=60)]: Done 500 out of 500 | elapsed:   24.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.999998982557525 %, derzeit: 31451408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend ThreadingBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  80 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=60)]: Done 330 tasks      | elapsed:   17.8s\n",
      "[Parallel(n_jobs=60)]: Done 500 out of 500 | elapsed:   25.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.999998728196907 %, derzeit: 39314260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend ThreadingBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  80 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=60)]: Done 330 tasks      | elapsed:   17.4s\n",
      "[Parallel(n_jobs=60)]: Done 500 out of 500 | elapsed:   24.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.99999847383629 %, derzeit: 47177112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend ThreadingBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  80 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=60)]: Done 330 tasks      | elapsed:   17.8s\n",
      "[Parallel(n_jobs=60)]: Done 500 out of 500 | elapsed:   25.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.99999821947567 %, derzeit: 55039964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend ThreadingBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  80 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=60)]: Done 330 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=60)]: Done 500 out of 500 | elapsed:   24.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.99999796511505 %, derzeit: 62902816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend ThreadingBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  80 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=60)]: Done 330 tasks      | elapsed:   18.3s\n",
      "[Parallel(n_jobs=60)]: Done 500 out of 500 | elapsed:   25.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.999997710754435 %, derzeit: 70765668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend ThreadingBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  80 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=60)]: Done 330 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=60)]: Done 500 out of 500 | elapsed:   25.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.999997456393814 %, derzeit: 78628520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend ThreadingBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  80 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=60)]: Done 330 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=60)]: Done 500 out of 500 | elapsed:   26.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.9999972020332 %, derzeit: 86491372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend ThreadingBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  80 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=60)]: Done 330 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=60)]: Done 500 out of 500 | elapsed:   26.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.99999694767258 %, derzeit: 94354224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend ThreadingBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  80 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=60)]: Done 330 tasks      | elapsed:   18.4s\n",
      "[Parallel(n_jobs=60)]: Done 500 out of 500 | elapsed:   26.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.99999669331196 %, derzeit: 102217076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend ThreadingBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  80 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=60)]: Done 330 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=60)]: Done 500 out of 500 | elapsed:   26.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.99999643895134 %, derzeit: 110079928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend ThreadingBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  80 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=60)]: Done 330 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=60)]: Done 500 out of 500 | elapsed:   25.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.99999618459073 %, derzeit: 117942780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend ThreadingBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  80 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=60)]: Done 330 tasks      | elapsed:   17.7s\n",
      "[Parallel(n_jobs=60)]: Done 500 out of 500 | elapsed:   25.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.9999959302301 %, derzeit: 125805632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend ThreadingBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  80 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=60)]: Done 330 tasks      | elapsed:   17.7s\n",
      "[Parallel(n_jobs=60)]: Done 500 out of 500 | elapsed:   24.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.99999567586949 %, derzeit: 133668484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend ThreadingBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  80 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=60)]: Done 330 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=60)]: Done 500 out of 500 | elapsed:   25.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.99999542150887 %, derzeit: 141531336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend ThreadingBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  80 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=60)]: Done 330 tasks      | elapsed:   16.7s\n",
      "[Parallel(n_jobs=60)]: Done 500 out of 500 | elapsed:   23.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.99999516714824 %, derzeit: 149394188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend ThreadingBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  80 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=60)]: Done 330 tasks      | elapsed:   17.2s\n",
      "[Parallel(n_jobs=60)]: Done 500 out of 500 | elapsed:   24.7s finished\n",
      "[Parallel(n_jobs=60)]: Using backend ThreadingBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  80 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=60)]: Done 330 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=60)]: Done 500 out of 500 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.99999491278763 %, derzeit: 157257040\n",
      "Class prediction was successful without slicing!\n",
      "Reshaped back to (11908, 13206)\n",
      "Image saved to: /work/pi_cschweik_umass_edu/Ryan/Classification_products/OTH_all_reclass_classification_v3_s2.gtif\n"
     ]
    }
   ],
   "source": [
    "\n",
    "slices = int(round(len(img)/20))\n",
    "\n",
    "test = True\n",
    "\n",
    "while test == True:\n",
    "    try:\n",
    "        class_preds = list()\n",
    "\n",
    "        temp = rf.predict(img[0:slices+1,:])\n",
    "        class_preds.append(temp)\n",
    "\n",
    "        for i in range(slices,len(img),slices):\n",
    "            print('{} %, derzeit: {}'.format((i*100)/(len(img)), i))\n",
    "            temp = rf.predict(img[i+1:i+(slices+1),:])                \n",
    "            class_preds.append(temp)\n",
    "\n",
    "    except MemoryError as error:\n",
    "        slices = slices/4\n",
    "        print('Not enought RAM, new slices = {}'.format(slices))\n",
    "\n",
    "    else:\n",
    "        test = False\n",
    "else:\n",
    "    print('Class prediction was successful without slicing!')\n",
    "#concatenate all slices and re-shape it to the original extend\n",
    "try:\n",
    "    class_prediction = np.concatenate(class_preds,axis = 0)\n",
    "except NameError:\n",
    "    print('No slicing was necessary!')\n",
    "    \n",
    "class_prediction = class_prediction.reshape(old_shape[:2])\n",
    "print('Reshaped back to {}'.format(class_prediction.shape))\n",
    "\n",
    "\n",
    "# # generate mask image from red band\n",
    "# mask = np.copy(img[:,:,0])\n",
    "# mask[mask > 0.0] = 1.0 # all actual pixels have a value of 1.0\n",
    "\n",
    "# plot mask\n",
    "\n",
    "# plt.imshow(mask)\n",
    "\n",
    "# mask classification an plot\n",
    "\n",
    "class_prediction.astype(np.float16)\n",
    "class_prediction_ = class_prediction*mask\n",
    "\n",
    "cols = class_prediction.shape[1]\n",
    "rows = class_prediction.shape[0]\n",
    "\n",
    "class_prediction_.astype(np.float16)\n",
    "\n",
    "driver = gdal.GetDriverByName(\"gtiff\")\n",
    "outdata = driver.Create(classification_image, cols, rows, 1, gdal.GDT_UInt16)\n",
    "outdata.SetGeoTransform(img_ds.GetGeoTransform())##sets same geotransform as input\n",
    "outdata.SetProjection(img_ds.GetProjection())##sets same projection as input\n",
    "outdata.GetRasterBand(1).WriteArray(class_prediction_)\n",
    "outdata.FlushCache() ##saves to disk!!\n",
    "del outdata\n",
    "print('Image saved to: {}'.format(classification_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Salt Marsh",
   "language": "python",
   "name": "salt_marsh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
