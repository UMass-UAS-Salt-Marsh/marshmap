---
title: "Field data summary"
author: "Brad Compton"
date: "`r Sys.Date()`"
output: html_document
---
<style>
/* Limit body text width, center paragraphs and lists */
.main-container p,
.main-container ul,
.main-container ol,
.main-container blockquote {
max-width: 850px;
margin-left: auto;
margin-right: auto;
}

/* Do NOT constrain code output or code blocks */
.main-container pre,
.main-container pre.r,
.main-container code,
.main-container .r-output {
max-width: none !important;
}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment = NA)
options(width = 120)

library(marshmap)
```

# Field data goals for U-Net
- Preliminary guess is that we'll want >10k cells per subclass for robust fits
- We're cross-validating by holding out entire polygons (transects, MUs, or polys) for validation. For the years we model, we'll 
  need at least 10 (preferably more) polys per subclass. These should be scattered across the site--both separated by >20 m or so,
  and ideally spanning the entire study area.
- Some subclasses are rare in some or all sites. We'll have to drop these as necessary because of insufficient sample sizes. For 
  preliminary fitting, we can just drop them, but when producing maps, we'll need to lump them so they map reasonably.
- Some subclasses are highly emphemeral, especially 32 wrack. We may need to drop these. They'll present problems for mapping if we do.
- We obviously can't do much with data previously collected in the field at this point, but should be able to tailor PI accordingly.
- Because the marsh changes over time, we'll want to pick a single year of field data and match orthos to it (within a year or so)
- Some sites (OTH, ESS, others?) have had significant remediation efforts coinciding with our study, so we may not be able to model 
  for these sites. If we can find out dates that work was done, we may be able to work around it (e.g., if it started in 2022)
- For U-Net modeling, I'm going to pick a couple of sites and years to start with. We'll need strong data for those years.
- My inclination is to pick strong candidate sites and years, and focus beefing up PI at these sites where necessary and possible.

## Still needed
- Not all sites have been completed for 2025
- We still need years for both field and PI data at most sites. Note the `0` year in tables below.
- It's possible we could correct for insufficient polygons by breaking long field transects into separate polygons, but we'd probably
  have to delete ~20 m sections to avoid contaminating validation sets. I may be able to do this in code on the fly.



# OTH (Old Town Hill)

```{r}
fieldinfo('oth')
```

## Notes
- Active management at this site
- Have years for field and PI data, but missing for subclasses 1, 3, 9, 12 (are these all really possible from PI?!)
- 2018 and 2021 look reasonably promising (except for mowing in the high marsh)


# ESS (Essex Bay)

```{r}
# fieldinfo('ess')
```

## Notes
- Active management at this site
- No field data yet


# PEG (Peggotty Beach)

```{r}
fieldinfo('peg')
```

## Notes
- Unclear how promising this site is until we have dates


# SOR (South River)

```{r}
# fieldinfo('sor')
```

## Notes
- No field data yet


# NOR (North River)

```{r}
#fieldinfo('nor')
```

## Notes
- No field data yet


# WEL (Wellfleet Bay)

```{r}
fieldinfo('wel')
```

## Notes



# BAR (Barnstable)

```{r}
#fieldinfo('bar')
```

## Notes
- No field data yet


# RR (Red River)

```{r}
fieldinfo('rr')
```

## Notes



# WES (Westport)

```{r}
#fieldinfo('wes')
```

## Notes
- No field data yet






