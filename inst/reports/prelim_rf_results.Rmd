---
title: "Preliminary random forest results"
author: "Brad Compton"
date: "`r Sys.Date()`"
output: html_document
---
<style>
/* Limit body text width, center paragraphs and lists */
.main-container p,
.main-container ul,
.main-container ol,
.main-container blockquote {
  max-width: 850px;
  margin-left: auto;
  margin-right: auto;
}

/* Do NOT constrain code output or code blocks */
.main-container pre,
.main-container pre.r,
.main-container code,
.main-container .r-output {
  max-width: none !important;
}
</style>



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment = NA)

modelsdir <- 'C:/Work/etc/saltmarsh/models/' 
predictdir <- 'c:/Work/etc/saltmarsh/data/oth/gis/predicted'
model <- 'fit_oth_2025-Apr-25_15-52'
predicted <- 'predicted'

model_fit <- readRDS(file.path(modelsdir, paste0(model, '.RDS')))
prediction <- terra::rast(file.path(predictdir, paste0(predicted, '.tif')))
```

## Initial results

These results are brand-new: I did my first random forest model run on April 23. I've only run a small 
handful of models for one site, Old Town Hill. Here are some results of a representative run. I'm 
surprised at how well they're fitting: correct classification rates are in the mid-90s, which is superb
for a start.

### Model info

Model name: `r model`  
`r dim(model_fit$train)[2]` variables  
n = `r dim(model_fit$train)[1]` (training), `r dim(model_fit$validate)[1]` (validation)  

Correct classification rate (CCR) = `r round(model_fit$confuse$overall['Accuracy'] * 100, 2)`%  
Kappa = `r round(model_fit$confuse$overall['Kappa'], 4)`  

### Confusion matrix and detailed stats

```{r}
print(model_fit$confuse)
```

#### Explanations

- The confusion matrix and all stats are based on the holdout set.
- The confusion matrix shows which classification errors were made. Values falling on the 
  diagonal were predicted correctly.
- Accuracy is the correct classification rate (also known as CCR), the proportion of cases that fall on the 
  diagonal in the confusion matrix.
- Kappa is a refined version of the CCR that takes the probability of chance agreement into account.
- The No Information Rate is the CCR you'd get if you always bet the majority class. In this particular 
  example, I've balanced the samples (subject to random sampling), so the NIR isn't particularly relevant.
- For each class, we get 
  - Sensitivity, true positives / all positives
  - Specificity, true negatives / all negatives
  - Positive Predicted Value, the proportion of positives that are true positives
  - Negative Predicted Value, the proportion of negatives that are true negatives
  - Prevalence, the proportion of cases that are in this class
  - Detection Rate, the proportion of cases that are true positives
  - Detection Prevalence, the proportion of cases predicted as positive
  - Balanced Accuracy, mean of sensitivity and selectivity
  
#### Comments

Overall, the CCR and other statistics look superb. I'm surprised at how well the model is fitting, even
with relatively small sample sizes. We can use the by-class statistics to look for problem classes, though
none of them look bad in this example. We're fitting the transitional marsh subclasses (3, 4, 5) well. 
The model even does well with small-sample size classes such as 7 and 12


## AUC and F1

**I'd like to implement these and include them.**

## Variable importance

This plot summarizes overall variable importance. Only the most important 20 variables are shown.

```{r}
plot(model_fit$import)
```

#### Comments

- As you can see, a handful of variables do the heavy lifting, and many are likely not worth 
  including. 
- Note that a high tide image doesn't come in until the 6th most important variable,
  indicating that the fit is driven primarily by topography and vegetation rather than directly
  by hydrology. 
- This particular fit has 4 photogrammetry-based DEMs in high positions, with one
  early-season, two late season, and one post-season. Other runs I've tried have an early-late 
  pair. I interpret this as the model using this combination to get at canopy height. I'd had 
  high hopes for the LiDAR-based canopy height model, but it comes in quite low--the model is 
  preferring photogrammetry-based versions instead.
- The most important variable is a short-wave infrared image, and RGB images don't come in until
  the 7th place. I've seen SWIR rate as highly important in other runs.

## Example map

```{r}
terra::plot(prediction)
```

## Comments and caveats
1. These results are super-preliminary. I expect model fits will generally improve as I refine models, perhaps
   by a lot. The only obvious places where we're likely to see declines in performance are if it turns out we
   have a spatial autocorrelation problem, and doubtless when we apply a model built on one site to another
   site.
2. These models are based on only 20% of the field points (mostly because I wanted fitting to run fast during
   development).
3. I built the models on 80% of the selected data points (so 80% of 20% of all pixels in field transects), and 
   held out 20% of points for validation. All stats are based on the holdout validation set.
4. I haven't tackled spatial autocorrelation yet. I'm concerned that autocorrelation could be contaminating
   the holdout sets. This may not be a thing, but I want to be sure.
5. I've done some minimal tuning of hyperparmeters, but will do more.
6. Field-based transects are not representative, as several classes were photointerpreted in the lab to get
   sample sizes up. I'm currently balancing representation, as we don't have true representation. I'll probably
   end up using both field-based and PI-based data, but weighting cases by field-based representation so I
   can drop balancing.
7. I'm currently ignoring the years field transects and UAS imagery were collected. Will want to
   put some effort into matching them up better (or at least let the data tell us if they matter).

## Next steps
1. Add derived variables such as NDVI, NWDI, NDRE.
2. Try upscaling variables at several scales, taking focal mean, SD, IQR, maybe 10th and 90th percentile.
3. Assess spatial autocorrelation and decide how sparsely we need to sample the data. Run with more complete
   datasets if that's the thing to do.
4. Multi-stage modeling: try targeting lumped subclasses and then spit these in subsequent models to see if 
   that improves fits. I expect to see performance improvements if we target a smaller number of classes.
5. Try alternatives to random forest. I'm currently planning to try AdaBoost, but there are other options.
   I'm using the `caret` framework, which makes it easy to switch modeling approaches, and there are hundreds
   of choices.
6. Write the model launching and tracking framework I've envisioned, which will make it easy to launch models
   in parallel as batch processes on Unity, and track model results over time. This will make it possible to 
   run many different models and do preliminary evaluation without producing maps of results. It'll then be 
   straightforward to produce maps of promising models. A setup like this will be necessary for running more
   than a handful of models.
7. Make data sampling more flexible.
8. Implement a macro naming system for variables so we can use common names across sites (e.g., 
   `21Aug20_OTH_MidOut_SWIR_Ortho` from Old Town Hill and `17Sep22_RED_Mid_SWIR_Ortho` from Red River could 
   both be referred to as `Late_Mid_SWIR_Ortho`. This will be necessary for cross-site modeling.
9. Produce models for other sites as Ryan finishes preparing transect data. Peggoty, Red River, Wellfleet,
   and Westport should be ready soon.
9. Try applying models across sites and building models for multiple sites in a cross-validation framework.
