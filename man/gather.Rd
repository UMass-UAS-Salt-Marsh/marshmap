% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gather.R
\name{gather}
\alias{gather}
\title{Gather and prepare GIS data from data sources}
\usage{
gather(
  site,
  pattern = "",
  update = TRUE,
  check = FALSE,
  field = TRUE,
  ignore_bad_classes = FALSE,
  replace_caches = FALSE,
  resources = NULL,
  local = FALSE,
  trap = TRUE,
  comment = NULL
)
}
\arguments{
\item{site}{One or more site names, using 3 letter abbreviation. Use \code{all} to process all sites.
In batch mode, each named site will be run in a separate job.}

\item{pattern}{Regex filtering rasters, case-insensitive. Default = "" (match all). Note: only
files ending in \code{.tif} are included in any case.
Examples:
\itemize{
\item to match all Mica orthophotos, use \code{mica_orth}
\item to match all Mica files from July, use \code{Jun.*mica}
\item to match Mica files for a series of dates, use \verb{11nov20.*mica|14oct20.*mica}
}}

\item{update}{If TRUE, only process new files, assuming existing files are good; otherwise,
process all files and replace existing ones.}

\item{check}{If TRUE, just check to see that source directories and files exist, but don't
cache or process anything}

\item{field}{If TRUE, download and process the field transects if they don't already exist.
The shapefile is downloaded for reference, and a raster corresponding to \code{standard} is created.}

\item{ignore_bad_classes}{If TRUE, don't throw an error if there are classes in the ground
truth shapefile that don't occur in \code{classes.txt}. Only use this if you're paying careful
attention, because bad classes will crash \code{do_map} down the line.}

\item{replace_caches}{If TRUE, all cached images (used for \code{screen}) are replaced}

\item{resources}{Slurm launch resources. See \link[slurmcollie]{launch}. These take priority
#'    over the function's defaults.}

\item{local}{If TRUE, run locally; otherwise, spawn a batch run on Unity}

\item{trap}{If TRUE, trap errors in local mode; if FALSE, use normal R error handling. Use this
for debugging. If you get unrecovered errors, the job won't be added to the jobs database. Has
no effect if local = FALSE.}

\item{comment}{Optional slurmcollie comment}
}
\description{
Gather raster and vector data from the source (either Google Drive or SFTP), and clip to site
boundary, resample and align to standard resolution. Data will be copied from various source
locations (orthophotos, DEMs, canopy height models). Robust to crashes and interruptions: cached
datasets that are fully downloaded will be used over re-downloading, and processed rasters won't be
re-processed unless \code{update = TRUE} or \code{replace = TRUE}.
}
\details{
Additional parameters, set in the \code{gather} block in \code{pars.yml} (see \code{\link[=init]{init()}}):
\itemize{
\item \code{sourcedrive} one of \code{local}, \code{google}, \code{sftp}
\itemize{
\item \code{local} - read source from local drive
\item \code{google} - get source data from currently connected Google Drive (login via browser on first connection)
and cache it locally. Must set \code{cachedir} option.
\item \code{sftp} - get source data from SFTP site. Must set \code{sftp} and \code{cachedir} options.
}
\item \code{sourcedir} directory with source rasters, generally on Google Drive or SFTP site
\item \code{subdirs} subdirectories to search, ending with slash. Default = orthos, DEMs, and canopy height models (okay
to include empty or nonexistent directories). Use \verb{<site>} in subdirectories that include a site name, e.g.,
\verb{<site> Share/Photogrammetry DEMs}. WARNING: paths on the Google Drive are case-sensitive!
\item \code{transects} directory with field transect shapefile
\item \code{exclude} list of geoTIFFs to exclude, for whatever reasons. Note that files beginning with \code{bad} are also
excluded
\item \code{sftp} \verb{list(url = <address of site>, user = <credentials>)}. Credentials are either \code{username:password} or
\verb{*filename} with \code{username:password}. Make sure
to include credential files in \code{.gitignore} and \code{.Rbuildignore} so it doesn't end up out in the world!
}

Source data:
\itemize{
\item geoTIFFs for each site
\item \code{sites} file, table of site abbreviation, site name, footprint shapefile, raster standard, and transect
shapefile.
}

Results:
\itemize{
\item flights/geoTIFFs, clipped, resampled, and aligned. \emph{\strong{Make sure you've closed ArcGIS/QGIS projects that
point to these before running!}}
\item models/gather_data.log
}

All source data are expected to be in \code{EPSG:4326}. Non-conforming rasters will be reprojected.

\code{sites.txt} must include the name of the footprint shapefile for each site, a field transect
shapefile, and a standard geoTIFF for each site. The footprint is used for clipping and must be
present. The transect contains ground truth data, and must be present if \code{field = TRUE}. The
standard must be present. It is used as the standard for grain and alignment; all rasters will be
resampled to match. Standards MUST be in the standard projection, \code{EPSG:4326}. Best to use a Mica
orthophoto, with 8 cm resolution.

Note that adding to an existing stack using a different standard will lead to sorrow. \strong{BEST
PRACTICE}: don't change the standards in \code{standards.txt}; if you must change them, clear the
flights/ directory and rerun.

If you're reading from the Google Drive or SFTP, you'll need a cache. Best to put this on the
Unity \strong{scratch drive}. Create it with \verb{ws_allocate cache 30} in the Unity shell. You can extend
the scratch drive (up to 5 times) with \verb{ws_extend cache 30}. When you're done with it, be polite
and release it with \verb{ws_release cache}. You'll need to point to the cache in \verb{~/pars.yml}, under
\verb{scratchdir:}.

Note that initial runs with Google Drive in a session open the browser for authentication or wait
for input from the console, so don't run blindly when using the Google Drive

At the end of a run, the log file will be copied to the flights directory.

Remember that some SFTP servers require connection via VPN

Example runs:

Complete for all sites:

\if{html}{\out{<div class="sourceCode">}}\preformatted{  `gather('all')`
}\if{html}{\out{</div>}}

Run for one site, June only:

\if{html}{\out{<div class="sourceCode">}}\preformatted{  `gather(site = 'oth', pattern = 'Jun')`
}\if{html}{\out{</div>}}

Run for 2 sites, low tide only:

\if{html}{\out{<div class="sourceCode">}}\preformatted{  `gather(site = c('oth', 'wes'), pattern = '_low_')`
}\if{html}{\out{</div>}}
}
