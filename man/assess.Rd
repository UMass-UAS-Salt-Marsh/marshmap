% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/assess.R
\name{assess}
\alias{assess}
\title{Assess a model fit from validation data}
\usage{
assess(
  fitid = NULL,
  model = NULL,
  newdata = NULL,
  site = NULL,
  top_importance = 20,
  summary = TRUE,
  confusion = TRUE,
  importance = TRUE,
  quiet = FALSE
)
}
\arguments{
\item{fitid}{id of a model in the fits database. If using this, omit \code{model}, as
this info will be extracted from the database.}

\item{model}{Only when called by do_fit; named list of:
\describe{
\item{fit}{model fit oject}
\item{confuse}{Confusion matrix}
\item{nvalidate}{Number of cases in validation set}
\item{id}{Model id}
\item{name}{Model name}
}}

\item{newdata}{An alternate validation set (e.g., from a different site). Variables
must conform with the original dataset.}

\item{site}{One or more site names, for display only}

\item{top_importance}{Number of variables to keep for variable importance}

\item{summary}{Print model summary info if TRUE}

\item{confusion}{Print the confusion matrix and complete statistics if TRUE, and skip if FALSE}

\item{importance}{Print variable importance if TRUE, and skip printing if FALSE}

\item{quiet}{If TRUE, don't print anything; just silently return stuff}
}
\value{
Invisibly, a named list of
\describe{
\item{confusion}{Confusion matrix and complete statistics}
\item{importance}{Variable importance data frame}
}
}
\description{
Provide a model assessment. Normally called by \code{fit}, but may be called separately for models applied to new sites.
}
\details{
Called by \code{do_fit}, but also may be called by the user. Either provide \code{fitid} for
the model you want to assess (the normal approach), or \code{model}, a list with necessary
arguments (the approach used by \code{do_fit}, becaue the model is not yet in the database).
When you call \code{assess} from the console, the fits database is not updated with the new
assessment.

You may supply \code{newdata} to assess a model on sites different from what the model
was built on. \code{newdata} is a data frame that conforms to the data the model was
built on. (\emph{\strong{how exactly}}?)

Assessments are returned invisibly; by default, they are printed to the console.

\strong{Explanations}

\emph{\strong{1. Model info}}
\itemize{
\item Model fit id and name, if supplied
\item Number of variables fit
\item Sample size for training and validation holdout set. The confusion matrix and all statistics are
derived from the holdout set.
\item Correct classification rate, the percent of cases that were predicted correctly.
\item Kappa, a refined version of the CCR that takes the probability of chance agreement into account.
}

\emph{\strong{2. Confusion matrix}}
\itemize{
\item Shows which classification errors were made. Values falling on the diagonal were predicted correctly.
}

\emph{\strong{3. Overall statistics}}
\itemize{
\item \emph{Accuracy} is the correct classification rate (also known as CCR), the percent of cases that fall on
the diagonal in the confusion matrix.
\item The \emph{No Information Rate} is the CCR you'd get if you always bet the majority class.
\item \emph{Kappa} is a refined version of the CCR that takes the probability of chance agreement into account.
\item \emph{Mcnemar's test} only applies to two-class data.
}

\emph{\strong{4. Statistics by class}}
\itemize{
\item Lists the follwing statistics for each of the subclasses.
These all scale from 0 to 1, with 1 generally indicating higher performance (except for prevalence,
detection rate, and detection prevalence).
\itemize{
\item \emph{Precision}, the proportion of cases predicted to be in the class that actually were (true positives /
(true positives + false positives))
\item \emph{Recall}, the proportion of cases actually in the class that were predicted to be in the class (true positives /
(true positives + false negatives))
\item \emph{F1}, the harmonic mean of precision and recall; a combined metric of model performance
\item \emph{Prevalence}, the proportion of all cases that are in this class
\item \emph{Detection Rate}, the proportion of all cases that are correctly predicted to be in this class
\item \emph{Detection Prevalence}, the proportion of all cases predicted to be in this class
\item \emph{Balanced Accuracy}, mean of true positive rate and true negative rate; a combined metric of model performance
\item \emph{AUC} (Area Under the Curve) is the probability that the model, for a particular class, when given a
random case in the class and a random case from another class, will rate the case in the class higher.
Unlike the other statistics, AUC is independent of the particluar cutpoint chosen, and is telling us
about the performance of the probabilities produced by the model.
}
}

\emph{\strong{5. Variable importance}}
\itemize{
\item Scaled from 0 to 100, gives the relative contribution of each variable to the model fit. Less-important variables
will be trimmed based on the top_importance option. Note that variables are imagery bands, not an entire orthoimage;
thus, for instance, an RGB true color image represents three varaibles, any of which may come into the model separately.
}
}
